
@article{Smithinventoryactivesubglacial2009,
  langid = {english},
  title = {An Inventory of Active Subglacial Lakes in {{Antarctica}} Detected by {{ICESat}} (2003–2008)},
  volume = {55},
  issn = {0022-1430, 1727-5652},
  url = {https://www.cambridge.org/core/product/identifier/S0022143000207399/type/journal_article},
  doi = {10.3189/002214309789470879},
  number = {192},
  journaltitle = {Journal of Glaciology},
  urldate = {2018-01-10},
  date = {2009},
  pages = {573-595},
  author = {Smith, Benjamin E. and Fricker, Helen A. and Joughin, Ian R. and Tulaczyk, Slawek}
}

@article{WrightfourthinventoryAntarctic2012,
  langid = {english},
  title = {A Fourth Inventory of {{Antarctic}} Subglacial Lakes},
  volume = {24},
  issn = {0954-1020, 1365-2079},
  url = {http://www.journals.cambridge.org/abstract_S095410201200048X},
  doi = {10.1017/S095410201200048X},
  number = {06},
  journaltitle = {Antarctic Science},
  urldate = {2018-01-10},
  date = {2012-12},
  pages = {659-664},
  author = {Wright, Andrew and Siegert, Martin}
}

@article{RedmonYOLO9000BetterFaster2016,
  langid = {english},
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  url = {https://arxiv.org/abs/1612.08242},
  shorttitle = {{{YOLO9000}}},
  urldate = {2018-01-10},
  date = {2016-12-25},
  author = {Redmon, Joseph and Farhadi, Ali}
}

@article{SilverSoftwaresimplified2017,
  title = {Software Simplified},
  volume = {546},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/546173a},
  doi = {10.1038/546173a},
  number = {7656},
  journaltitle = {Nature},
  urldate = {2018-01-10},
  date = {2017-05-29},
  pages = {173-174},
  author = {Silver, Andrew}
}

@article{BoettigerintroductionDockerreproducible2015,
  langid = {english},
  title = {An Introduction to {{Docker}} for Reproducible Research},
  volume = {49},
  issn = {01635980},
  url = {http://dl.acm.org/citation.cfm?doid=2723872.2723882},
  doi = {10.1145/2723872.2723882},
  number = {1},
  journaltitle = {ACM SIGOPS Operating Systems Review},
  urldate = {2018-01-10},
  date = {2015-01-20},
  pages = {71-79},
  author = {Boettiger, Carl}
}

@article{CrickReproducibilityResearchSystems2017,
  langid = {english},
  title = {Reproducibility in {{Research}}: {{Systems}}, {{Infrastructure}}, {{Culture}}},
  volume = {5},
  issn = {2049-9647},
  url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.73/},
  doi = {10.5334/jors.73},
  shorttitle = {Reproducibility in {{Research}}},
  journaltitle = {Journal of Open Research Software},
  urldate = {2018-01-14},
  date = {2017-11-09},
  author = {Crick, Tom and Hall, Benjamin A. and Ishtiaq, Samin}
}

@article{PerkelDemocraticdatabasesscience2016,
  title = {Democratic Databases: Science on {{GitHub}}},
  volume = {538},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/538127a},
  doi = {10.1038/538127a},
  shorttitle = {Democratic Databases},
  number = {7623},
  journaltitle = {Nature},
  urldate = {2018-01-15},
  date = {2016-10-03},
  pages = {127-128},
  author = {Perkel, Jeffrey}
}

@article{BlischakQuickIntroductionVersion2016,
  langid = {english},
  title = {A {{Quick Introduction}} to {{Version Control}} with {{Git}} and {{GitHub}}},
  volume = {12},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1004668},
  doi = {10.1371/journal.pcbi.1004668},
  number = {1},
  journaltitle = {PLOS Computational Biology},
  urldate = {2018-01-15},
  date = {2016-01-19},
  pages = {e1004668},
  author = {Blischak, John D. and Davenport, Emily R. and Wilson, Greg},
  editor = {Ouellette, Francis}
}

@software{OgdenDatDistributedDataset2018,
  title = {Dat  - {{Distributed Dataset Synchronization}} and {{Versioning}}},
  url = {https://github.com/datprotocol/whitepaper/blob/master/dat-paper.pdf},
  version = {2.0},
  publisher = {{Datproject}},
  urldate = {2018-01-15},
  date = {2018-01-12},
  author = {Ogden, Maxwell and McKelvey, Karissa and Madsen, Mathias Buus and {Code for Science}},
  origdate = {2018-01-11T19:09:45Z}
}

@article{MykletunAuthenticationintegrityoutsourced2006,
  langid = {english},
  title = {Authentication and Integrity in Outsourced Databases},
  volume = {2},
  issn = {15533077},
  url = {http://portal.acm.org/citation.cfm?doid=1149976.1149977},
  doi = {10.1145/1149976.1149977},
  number = {2},
  journaltitle = {ACM Transactions on Storage},
  urldate = {2018-01-15},
  date = {2006-05-01},
  pages = {107-138},
  author = {Mykletun, Einar and Narasimha, Maithili and Tsudik, Gene}
}

@incollection{AumassonBLAKE2SimplerSmaller2013,
  location = {{Berlin, Heidelberg}},
  title = {{{BLAKE2}}: {{Simpler}}, {{Smaller}}, {{Fast}} as {{MD5}}},
  volume = {7954},
  isbn = {978-3-642-38979-5 978-3-642-38980-1},
  url = {http://link.springer.com/10.1007/978-3-642-38980-1_8},
  shorttitle = {{{BLAKE2}}},
  booktitle = {Applied {{Cryptography}} and {{Network Security}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2018-01-15},
  date = {2013},
  pages = {119-135},
  author = {Aumasson, Jean-Philippe and Neves, Samuel and Wilcox-O’Hearn, Zooko and Winnerlein, Christian},
  editor = {Jacobson, Michael and Locasto, Michael and Mohassel, Payman and Safavi-Naini, Reihaneh},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  doi = {10.1007/978-3-642-38980-1_8}
}

@article{BernsteinHighspeedhighsecuritysignatures2012,
  langid = {english},
  title = {High-Speed High-Security Signatures},
  volume = {2},
  issn = {2190-8508, 2190-8516},
  url = {http://link.springer.com/10.1007/s13389-012-0027-1},
  doi = {10.1007/s13389-012-0027-1},
  number = {2},
  journaltitle = {Journal of Cryptographic Engineering},
  urldate = {2018-01-15},
  date = {2012-09},
  pages = {77-89},
  author = {Bernstein, Daniel J. and Duif, Niels and Lange, Tanja and Schwabe, Peter and Yang, Bo-Yin}
}

@incollection{MaymounkovKademliaPeertoPeerInformation2002,
  location = {{Berlin, Heidelberg}},
  title = {Kademlia: {{A Peer}}-to-{{Peer Information System Based}} on the {{XOR Metric}}},
  volume = {2429},
  isbn = {978-3-540-44179-3 978-3-540-45748-0},
  url = {http://link.springer.com/10.1007/3-540-45748-8_5},
  shorttitle = {Kademlia},
  booktitle = {Peer-to-{{Peer Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2018-01-15},
  date = {2002},
  pages = {53-65},
  author = {Maymounkov, Petar and Mazières, David},
  editor = {Druschel, Peter and Kaashoek, Frans and Rowstron, Antony},
  editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
  editorbtype = {redactor},
  options = {useprefix=true},
  doi = {10.1007/3-540-45748-8_5}
}

@article{CholletKeras2015,
  title = {Keras},
  url = {https://github.com/keras-team/keras},
  date = {2015},
  author = {Chollet, François and {others}}
}

@article{RonnebergerUNetConvolutionalNetworks2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.04597},
  primaryClass = {cs},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  url = {http://arxiv.org/abs/1505.04597},
  shorttitle = {U-{{Net}}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  urldate = {2018-01-26},
  date = {2015-05-18},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}
}

@article{LeCunBackpropagationAppliedHandwritten1989,
  langid = {english},
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  volume = {1},
  issn = {0899-7667, 1530-888X},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.541},
  doi = {10.1162/neco.1989.1.4.541},
  number = {4},
  journaltitle = {Neural Computation},
  urldate = {2018-01-26},
  date = {1989-12},
  pages = {541-551},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.}
}

@article{LecunGradientbasedlearningapplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {00189219},
  url = {http://ieeexplore.ieee.org/document/726791/},
  doi = {10.1109/5.726791},
  number = {11},
  journaltitle = {Proceedings of the IEEE},
  urldate = {2018-01-26},
  date = {1998-11},
  pages = {2278-2324},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.}
}

@article{KrizhevskyImageNetclassificationdeep2017,
  langid = {english},
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  volume = {60},
  issn = {00010782},
  url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
  doi = {10.1145/3065386},
  number = {6},
  journaltitle = {Communications of the ACM},
  urldate = {2018-01-26},
  date = {2017-05-24},
  pages = {84-90},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.}
}

@inproceedings{SteinkrausUsingGPUsmachine2005,
  title = {Using {{GPUs}} for Machine Learning Algorithms},
  isbn = {978-0-7695-2420-7},
  url = {http://ieeexplore.ieee.org/document/1575717/},
  doi = {10.1109/ICDAR.2005.251},
  publisher = {{IEEE}},
  urldate = {2018-01-26},
  date = {2005},
  pages = {1115-1120 Vol. 2},
  author = {Steinkraus, D. and Buck, I. and Simard, P.Y.}
}

@article{SimonyanVeryDeepConvolutional2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  url = {http://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  urldate = {2018-01-26},
  date = {2014-09-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew}
}

@article{GirshickRichfeaturehierarchies2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.2524},
  primaryClass = {cs},
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  url = {http://arxiv.org/abs/1311.2524},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde{}rbg/rcnn.},
  urldate = {2018-01-26},
  date = {2013-11-11},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}
}

@article{GirshickFastRCNN2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.08083},
  primaryClass = {cs},
  title = {Fast {{R}}-{{CNN}}},
  url = {http://arxiv.org/abs/1504.08083},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  urldate = {2018-01-26},
  date = {2015-04-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Girshick, Ross}
}

@article{RenFasterRCNNRealTime2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.01497},
  primaryClass = {cs},
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  url = {http://arxiv.org/abs/1506.01497},
  shorttitle = {Faster {{R}}-{{CNN}}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  urldate = {2018-01-26},
  date = {2015-06-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian}
}

@article{RedmonYouOnlyLook2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02640},
  primaryClass = {cs},
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  url = {http://arxiv.org/abs/1506.02640},
  shorttitle = {You {{Only Look Once}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  urldate = {2018-01-26},
  date = {2015-06-08},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali}
}

@article{HeMaskRCNN2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06870},
  primaryClass = {cs},
  title = {Mask {{R}}-{{CNN}}},
  url = {http://arxiv.org/abs/1703.06870},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  urldate = {2018-01-26},
  date = {2017-03-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross}
}

@article{LongFullyConvolutionalNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4038},
  primaryClass = {cs},
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  url = {http://arxiv.org/abs/1411.4038},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  urldate = {2018-01-28},
  date = {2014-11-14},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor}
}

@article{JegouOneHundredLayers2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.09326},
  primaryClass = {cs},
  title = {The {{One Hundred Layers Tiramisu}}: {{Fully Convolutional DenseNets}} for {{Semantic Segmentation}}},
  url = {http://arxiv.org/abs/1611.09326},
  shorttitle = {The {{One Hundred Layers Tiramisu}}},
  abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
  urldate = {2018-01-28},
  date = {2016-11-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua}
}

@article{VisinReNetRecurrentNeural2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.00393},
  primaryClass = {cs},
  title = {{{ReNet}}: {{A Recurrent Neural Network Based Alternative}} to {{Convolutional Networks}}},
  url = {http://arxiv.org/abs/1505.00393},
  shorttitle = {{{ReNet}}},
  abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
  urldate = {2018-01-29},
  date = {2015-05-03},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua}
}

@article{VisinReSegRecurrentNeural2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.07053},
  primaryClass = {cs},
  title = {{{ReSeg}}: {{A Recurrent Neural Network}}-Based {{Model}} for {{Semantic Segmentation}}},
  url = {http://arxiv.org/abs/1511.07053},
  shorttitle = {{{ReSeg}}},
  abstract = {We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg.},
  urldate = {2018-01-29},
  date = {2015-11-22},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Visin, Francesco and Ciccone, Marco and Romero, Adriana and Kastner, Kyle and Cho, Kyunghyun and Bengio, Yoshua and Matteucci, Matteo and Courville, Aaron}
}

@article{LiuSSDSingleShot2015,
  langid = {english},
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  url = {https://arxiv.org/abs/1512.02325v5},
  doi = {10.1007/978-3-319-46448-0_2},
  shorttitle = {{{SSD}}},
  urldate = {2018-01-29},
  date = {2015-12-08},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.}
}

@article{HeDeepResidualLearning2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  primaryClass = {cs},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  url = {http://arxiv.org/abs/1512.03385},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  urldate = {2018-01-29},
  date = {2015-12-10},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}
}

@article{HuangDenselyConnectedConvolutional2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06993},
  primaryClass = {cs},
  title = {Densely {{Connected Convolutional Networks}}},
  url = {http://arxiv.org/abs/1608.06993},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
  urldate = {2018-01-29},
  date = {2016-08-24},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and van der Maaten, Laurens},
  options = {useprefix=true}
}

@article{ZhangRoadExtractionDeep2017,
  langid = {english},
  title = {Road {{Extraction}} by {{Deep Residual U}}-{{Net}}},
  url = {https://arxiv.org/abs/1711.10684},
  urldate = {2018-01-29},
  date = {2017-11-29},
  author = {Zhang, Zhengxin and Liu, Qingjie and Wang, Yunhong}
}

@article{LiDeepUNetDeepFully2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.00201},
  primaryClass = {cs},
  title = {{{DeepUNet}}: {{A Deep Fully Convolutional Network}} for {{Pixel}}-Level {{Sea}}-{{Land Segmentation}}},
  url = {http://arxiv.org/abs/1709.00201},
  shorttitle = {{{DeepUNet}}},
  abstract = {Semantic segmentation is a fundamental research in remote sensing image processing. Because of the complex maritime environment, the sea-land segmentation is a challenging task. Although the neural network has achieved excellent performance in semantic segmentation in the last years, there are a few of works using CNN for sea-land segmentation and the results could be further improved. This paper proposes a novel deep convolution neural network named DeepUNet. Like the U-Net, its structure has a contracting path and an expansive path to get high resolution output. But differently, the DeepUNet uses DownBlocks instead of convolution layers in the contracting path and uses UpBlock in the expansive path. The two novel blocks bring two new connections that are U-connection and Plus connection. They are promoted to get more precise segmentation results. To verify our network architecture, we made a new challenging sea-land dataset and compare the DeepUNet on it with the SegNet and the U-Net. Experimental results show that DeepUNet achieved good performance compared with other architectures, especially in high-resolution remote sensing imagery.},
  urldate = {2018-01-29},
  date = {2017-09-01},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Li, Ruirui and Liu, Wenjie and Yang, Lei and Sun, Shihao and Hu, Wei and Zhang, Fan and Li, Wei}
}

@article{ChenDeepLabSemanticImage2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.00915},
  primaryClass = {cs},
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  url = {http://arxiv.org/abs/1606.00915},
  shorttitle = {{{DeepLab}}},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  urldate = {2018-01-29},
  date = {2016-06-02},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.}
}

@article{ChenRethinkingAtrousConvolution2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.05587},
  primaryClass = {cs},
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  url = {http://arxiv.org/abs/1706.05587},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  urldate = {2018-01-29},
  date = {2017-06-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig}
}

@article{SabourDynamicRoutingCapsules2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.09829},
  primaryClass = {cs},
  title = {Dynamic {{Routing Between Capsules}}},
  url = {http://arxiv.org/abs/1710.09829},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  urldate = {2018-01-29},
  date = {2017-10-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.}
}

@article{ZhuDeepLearningRemote2017,
  title = {Deep {{Learning}} in {{Remote Sensing}}: {{A Comprehensive Review}} and {{List}} of {{Resources}}},
  volume = {5},
  issn = {2168-6831, 2473-2397},
  url = {http://ieeexplore.ieee.org/document/8113128/},
  doi = {10.1109/MGRS.2017.2762307},
  shorttitle = {Deep {{Learning}} in {{Remote Sensing}}},
  number = {4},
  journaltitle = {IEEE Geoscience and Remote Sensing Magazine},
  urldate = {2018-01-31},
  date = {2017-12},
  pages = {8-36},
  author = {Zhu, Xiao Xiang and Tuia, Devis and Mou, Lichao and Xia, Gui-Song and Zhang, Liangpei and Xu, Feng and Fraundorfer, Friedrich}
}

@article{XiaDOTALargescaleDataset2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.10398},
  primaryClass = {cs},
  title = {{{DOTA}}: {{A Large}}-Scale {{Dataset}} for {{Object Detection}} in {{Aerial Images}}},
  url = {http://arxiv.org/abs/1711.10398},
  shorttitle = {{{DOTA}}},
  abstract = {Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect \$2806\$ aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using \$15\$ common object categories. The fully annotated DOTA images contains \$188,282\$ instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.},
  urldate = {2018-01-31},
  date = {2017-11-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Xia, Gui-Song and Bai, Xiang and Ding, Jian and Zhu, Zhen and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei}
}

@article{Liu3DCNNDQNRNNDeepReinforcement2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.06783},
  primaryClass = {cs},
  title = {{{3DCNN}}-{{DQN}}-{{RNN}}: {{A Deep Reinforcement Learning Framework}} for {{Semantic Parsing}} of {{Large}}-Scale {{3D Point Clouds}}},
  url = {http://arxiv.org/abs/1707.06783},
  shorttitle = {{{3DCNN}}-{{DQN}}-{{RNN}}},
  abstract = {Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN) for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.},
  urldate = {2018-01-31},
  date = {2017-07-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Liu, Fangyu and Li, Shuaipeng and Zhang, Liqiang and Zhou, Chenghu and Ye, Rongtian and Wang, Yuebin and Lu, Jiwen}
}

@article{MinhDeepRecurrentNeural2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.03694},
  primaryClass = {cs},
  title = {Deep {{Recurrent Neural Networks}} for Mapping Winter Vegetation Quality Coverage via Multi-Temporal {{SAR Sentinel}}-1},
  url = {http://arxiv.org/abs/1708.03694},
  abstract = {Mapping winter vegetation quality coverage is a challenge problem of remote sensing. This is due to the cloud coverage in winter period, leading to use radar rather than optical images. The objective of this paper is to provide a better understanding of the capabilities of radar Sentinel-1 and deep learning concerning about mapping winter vegetation quality coverage. The analysis presented in this paper is carried out on multi-temporal Sentinel-1 data over the site of La Rochelle, France, during the campaign in December 2016. This dataset were processed in order to produce an intensity radar data stack from October 2016 to February 2017. Two deep Recurrent Neural Network (RNN) based classifier methods were employed. We found that the results of RNNs clearly outperformed the classical machine learning approaches (Support Vector Machine and Random Forest). This study confirms that the time series radar Sentinel-1 and RNNs could be exploited for winter vegetation quality cover mapping.},
  urldate = {2018-01-31},
  date = {2017-08-11},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Minh, Dinh Ho Tong and Ienco, Dino and Gaetano, Raffaele and Lalande, Nathalie and Ndikumana, Emile and Osman, Faycal and Maurel, Pierre}
}

@article{IencoLandCoverClassification2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04055},
  title = {Land {{Cover Classification}} via {{Multi}}-Temporal {{Spatial Data}} by {{Recurrent Neural Networks}}},
  volume = {14},
  issn = {1545-598X, 1558-0571},
  url = {http://arxiv.org/abs/1704.04055},
  doi = {10.1109/LGRS.2017.2728698},
  abstract = {Nowadays, modern earth observation programs produce huge volumes of satellite images time series (SITS) that can be useful to monitor geographical areas through time. How to efficiently analyze such kind of information is still an open question in the remote sensing field. Recently, deep learning methods proved suitable to deal with remote sensing data mainly for scene classification (i.e. Convolutional Neural Networks - CNNs - on single images) while only very few studies exist involving temporal deep learning approaches (i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series. In this letter we evaluate the ability of Recurrent Neural Networks, in particular the Long-Short Term Memory (LSTM) model, to perform land cover classification considering multi-temporal spatial data derived from a time series of satellite images. We carried out experiments on two different datasets considering both pixel-based and object-based classification. The obtained results show that Recurrent Neural Networks are competitive compared to state-of-the-art classifiers, and may outperform classical approaches in presence of low represented and/or highly mixed classes. We also show that using the alternative feature representation generated by LSTM can improve the performances of standard classifiers.},
  number = {10},
  journaltitle = {IEEE Geoscience and Remote Sensing Letters},
  urldate = {2018-01-31},
  date = {2017-10},
  pages = {1685-1689},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Ienco, Dino and Gaetano, Raffaele and Dupaquier, Claire and Maurel, Pierre}
}

@article{ZhangProgressivelyDiffusedNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.05839},
  primaryClass = {cs},
  title = {Progressively {{Diffused Networks}} for {{Semantic Image Segmentation}}},
  url = {http://arxiv.org/abs/1702.05839},
  abstract = {This paper introduces Progressively Diffused Networks (PDNs) for unifying multi-scale context modeling with deep feature learning, by taking semantic image segmentation as an exemplar application. Prior neural networks, such as ResNet, tend to enhance representational power by increasing the depth of architectures and driving the training objective across layers. However, we argue that spatial dependencies in different layers, which generally represent the rich contexts among data elements, are also critical to building deep and discriminative representations. To this end, our PDNs enables to progressively broadcast information over the learned feature maps by inserting a stack of information diffusion layers, each of which exploits multi-dimensional convolutional LSTMs (Long-Short-Term Memory Structures). In each LSTM unit, a special type of atrous filters are designed to capture the short range and long range dependencies from various neighbors to a certain site of the feature map and pass the accumulated information to the next layer. From the extensive experiments on semantic image segmentation benchmarks (e.g., ImageNet Parsing, PASCAL VOC2012 and PASCAL-Part), our framework demonstrates the effectiveness to substantially improve the performances over the popular existing neural network models, and achieves state-of-the-art on ImageNet Parsing for large scale semantic segmentation.},
  urldate = {2018-01-31},
  date = {2017-02-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Zhang, Ruimao and Yang, Wei and Peng, Zhanglin and Wang, Xiaogang and Lin, Liang}
}

@article{YuDilatedResidualNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09914},
  primaryClass = {cs},
  title = {Dilated {{Residual Networks}}},
  url = {http://arxiv.org/abs/1705.09914},
  abstract = {Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model's depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (`degridding'), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.},
  urldate = {2018-01-31},
  date = {2017-05-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Yu, Fisher and Koltun, Vladlen and Funkhouser, Thomas}
}

@incollection{ZhangImageSegmentationPyramid2017,
  location = {{Cham}},
  title = {Image {{Segmentation}} with {{Pyramid Dilated Convolution Based}} on {{ResNet}} and {{U}}-{{Net}}},
  volume = {10635},
  isbn = {978-3-319-70095-3 978-3-319-70096-0},
  url = {http://link.springer.com/10.1007/978-3-319-70096-0_38},
  booktitle = {Neural {{Information Processing}}},
  publisher = {{Springer International Publishing}},
  urldate = {2018-01-31},
  date = {2017},
  pages = {364-372},
  author = {Zhang, Qiao and Cui, Zhipeng and Niu, Xiaoguang and Geng, Shijie and Qiao, Yu},
  editor = {Liu, Derong and Xie, Shengli and Li, Yuanqing and Zhao, Dongbin and El-Alfy, El-Sayed M.},
  doi = {10.1007/978-3-319-70096-0_38}
}

@article{HuangSpeedaccuracytradeoffs2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.10012},
  primaryClass = {cs},
  title = {Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors},
  url = {http://arxiv.org/abs/1611.10012},
  abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
  urldate = {2018-02-04},
  date = {2016-11-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin}
}

@article{RezvanbehbahaniPredictingGeothermalHeat2017,
  langid = {english},
  title = {Predicting the {{Geothermal Heat Flux}} in {{Greenland}}: {{A Machine Learning Approach}}: {{PREDICTION OF GHF IN GREENLAND}}},
  volume = {44},
  issn = {00948276},
  url = {http://doi.wiley.com/10.1002/2017GL075661},
  doi = {10.1002/2017GL075661},
  shorttitle = {Predicting the {{Geothermal Heat Flux}} in {{Greenland}}},
  number = {24},
  journaltitle = {Geophysical Research Letters},
  urldate = {2018-02-21},
  date = {2017-12-28},
  pages = {12,271-12,279},
  author = {Rezvanbehbahani, Soroush and Stearns, Leigh A. and Kadivar, Amir and Walker, J. Doug and van der Veen, C. J.},
  options = {useprefix=true}
}

@misc{JezekRAMPAMM1SAR2013,
  title = {{{RAMP AMM}}-1 {{SAR Image Mosaic}} of {{Antarctica}}, {{Version}} 2},
  publisher = {{NASA National Snow and Ice Data Center Distributed Active Archive Center}},
  date = {2013},
  author = {Jezek, Kenneth},
  doi = {10.5067/8AF4ZRPULS4H}
}

@article{ScambosMODISbasedMosaicAntarctica2007,
  langid = {english},
  title = {{{MODIS}}-Based {{Mosaic}} of {{Antarctica}} ({{MOA}}) Data Sets: {{Continent}}-Wide Surface Morphology and Snow Grain Size},
  volume = {111},
  issn = {00344257},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0034425707002854},
  doi = {10.1016/j.rse.2006.12.020},
  shorttitle = {{{MODIS}}-Based {{Mosaic}} of {{Antarctica}} ({{MOA}}) Data Sets},
  number = {2-3},
  journaltitle = {Remote Sensing of Environment},
  urldate = {2018-02-22},
  date = {2007-11},
  pages = {242-257},
  author = {Scambos, T.A. and Haran, T.M. and Fahnestock, M.A. and Painter, T.H. and Bohlander, J.}
}

@article{HelmElevationelevationchange2014,
  langid = {english},
  title = {Elevation and Elevation Change of {{Greenland}} and {{Antarctica}} Derived from {{CryoSat}}-2},
  volume = {8},
  issn = {1994-0424},
  url = {http://www.the-cryosphere.net/8/1539/2014/},
  doi = {10.5194/tc-8-1539-2014},
  number = {4},
  journaltitle = {The Cryosphere},
  urldate = {2018-02-22},
  date = {2014-08-20},
  pages = {1539-1559},
  author = {Helm, V. and Humbert, A. and Miller, H.}
}

@article{FretwellBedmap2improvedice2013,
  langid = {english},
  title = {Bedmap2: Improved Ice Bed, Surface and Thickness Datasets for {{Antarctica}}},
  volume = {7},
  issn = {1994-0424},
  url = {http://www.the-cryosphere.net/7/375/2013/},
  doi = {10.5194/tc-7-375-2013},
  shorttitle = {Bedmap2},
  number = {1},
  journaltitle = {The Cryosphere},
  urldate = {2018-02-22},
  date = {2013-02-28},
  pages = {375-393},
  author = {Fretwell, P. and Pritchard, H. D. and Vaughan, D. G. and Bamber, J. L. and Barrand, N. E. and Bell, R. and Bianchi, C. and Bingham, R. G. and Blankenship, D. D. and Casassa, G. and Catania, G. and Callens, D. and Conway, H. and Cook, A. J. and Corr, H. F. J. and Damaske, D. and Damm, V. and Ferraccioli, F. and Forsberg, R. and Fujita, S. and Gim, Y. and Gogineni, P. and Griggs, J. A. and Hindmarsh, R. C. A. and Holmlund, P. and Holt, J. W. and Jacobel, R. W. and Jenkins, A. and Jokat, W. and Jordan, T. and King, E. C. and Kohler, J. and Krabill, W. and Riger-Kusk, M. and Langley, K. A. and Leitchenkov, G. and Leuschen, C. and Luyendyk, B. P. and Matsuoka, K. and Mouginot, J. and Nitsche, F. O. and Nogi, Y. and Nost, O. A. and Popov, S. V. and Rignot, E. and Rippin, D. M. and Rivera, A. and Roberts, J. and Ross, N. and Siegert, M. J. and Smith, A. M. and Steinhage, D. and Studinger, M. and Sun, B. and Tinto, B. K. and Welch, B. C. and Wilson, D. and Young, D. A. and Xiangbin, C. and Zirizzotti, A.}
}

@article{LeBrocqEvidenceiceshelves2013,
  langid = {english},
  title = {Evidence from Ice Shelves for Channelized Meltwater Flow beneath the {{Antarctic Ice Sheet}}},
  volume = {6},
  issn = {1752-0894, 1752-0908},
  url = {http://www.nature.com/articles/ngeo1977},
  doi = {10.1038/ngeo1977},
  number = {11},
  journaltitle = {Nature Geoscience},
  urldate = {2018-02-22},
  date = {2013-11},
  pages = {945-948},
  author = {Le Brocq, Anne M. and Ross, Neil and Griggs, Jennifer A. and Bingham, Robert G. and Corr, Hugh F. J. and Ferraccioli, Fausto and Jenkins, Adrian and Jordan, Tom A. and Payne, Antony J. and Rippin, David M. and Siegert, Martin J.}
}

@misc{RignotMEaSUREsInSARBasedAntarctica2017,
  title = {{{MEaSUREs InSAR}}-{{Based Antarctica Ice Velocity Map}}, {{Version}} 2},
  publisher = {{NASA National Snow and Ice Data Center DAAC}},
  date = {2017},
  author = {Rignot, Eric},
  doi = {10.5067/D7GK8F5J8M8R}
}

@article{MartosHeatFluxDistribution2017,
  langid = {english},
  title = {Heat {{Flux Distribution}} of {{Antarctica Unveiled}}: {{Antarctic}} Heat Flux Unveiled},
  volume = {44},
  issn = {00948276},
  url = {http://doi.wiley.com/10.1002/2017GL075609},
  doi = {10.1002/2017GL075609},
  shorttitle = {Heat {{Flux Distribution}} of {{Antarctica Unveiled}}},
  number = {22},
  journaltitle = {Geophysical Research Letters},
  urldate = {2018-02-22},
  date = {2017-11-28},
  pages = {11,417-11,426},
  author = {Martos, Yasmina M. and Catalán, Manuel and Jordan, Tom A. and Golynsky, Alexander and Golynsky, Dmitry and Eagles, Graeme and Vaughan, David G.}
}

@article{SiegertSpatialvariationsheat1996,
  langid = {english},
  title = {Spatial Variations in Heat at the Base of the {{Antarctic}} Ice Sheet from Analysis of the Thermal Regime above Subglacial Lakes},
  volume = {42},
  issn = {0022-1430, 1727-5652},
  url = {https://www.cambridge.org/core/product/identifier/S0022143000003488/type/journal_article},
  doi = {10.3189/S0022143000003488},
  number = {142},
  journaltitle = {Journal of Glaciology},
  urldate = {2018-02-22},
  date = {1996},
  pages = {501-509},
  author = {Siegert, Martin J. and Dowdeswell, Julian A.}
}

@misc{LiuRadarsatAntarcticMapping2001,
  title = {Radarsat {{Antarctic Mapping Project Digital Elevation Model}}, {{Version}} 2},
  publisher = {{NASA National Snow and Ice Data Center DAAC}},
  date = {2001},
  author = {Liu, Hongxing},
  doi = {10.5067/8JKNEW6BFRVD}
}

@article{JezekGlaciologicalpropertiesAntarctic1999,
  langid = {english},
  title = {Glaciological Properties of the {{Antarctic}} Ice Sheet from {{RADARSAT}}-1 Synthetic Aperture Radar Imagery},
  volume = {29},
  issn = {0260-3055, 1727-5644},
  url = {https://www.cambridge.org/core/product/identifier/S0260305500267797/type/journal_article},
  doi = {10.3189/172756499781820969},
  journaltitle = {Annals of Glaciology},
  urldate = {2018-02-22},
  date = {1999},
  pages = {286-290},
  author = {Jezek, Kenneth C.}
}

@misc{TerryHaranMODISMosaicAntarctica2014,
  title = {{{MODIS Mosaic}} of {{Antarctica}} 2008-2009 ({{MOA2009}}) {{Image Map}}},
  url = {http://www.usap-dc.org/view/dataset/609593},
  publisher = {{U.S. Antarctic Program Data Center (USAP-DC), via National Snow and Ice Data Center (NSIDC)}},
  urldate = {2018-02-22},
  date = {2014},
  author = {Terry Haran, Jennifer Bohlander},
  doi = {10.7265/N5KP8037}
}

@misc{DimarzioGLASICESat5002007,
  title = {{{GLAS}}/{{ICESat}} 500 m {{Laser Altimetry Digital Elevation Model}} of {{Antarctica}}},
  publisher = {{NASA National Snow and Ice Data Center DAAC}},
  date = {2007},
  author = {Dimarzio, J.},
  doi = {10.5067/K2IMI0L24BRJ}
}

@article{ShumanICESatAntarcticelevation2006,
  langid = {english},
  title = {{{ICESat Antarctic}} Elevation Data: {{Preliminary}} Precision and Accuracy Assessment},
  volume = {33},
  issn = {0094-8276},
  url = {http://doi.wiley.com/10.1029/2005GL025227},
  doi = {10.1029/2005GL025227},
  shorttitle = {{{ICESat Antarctic}} Elevation Data},
  number = {7},
  journaltitle = {Geophysical Research Letters},
  urldate = {2018-02-22},
  date = {2006},
  author = {Shuman, C. A. and Zwally, H. J. and Schutz, B. E. and Brenner, A. C. and DiMarzio, J. P. and Suchdeo, V. P. and Fricker, H. A.}
}

@article{RignotIceFlowAntarctic2011,
  langid = {english},
  title = {Ice {{Flow}} of the {{Antarctic Ice Sheet}}},
  volume = {333},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1208336},
  doi = {10.1126/science.1208336},
  number = {6048},
  journaltitle = {Science},
  urldate = {2018-02-22},
  date = {2011-09-09},
  pages = {1427-1430},
  author = {Rignot, E. and Mouginot, J. and Scheuchl, B.}
}

@article{MouginotMappingIceMotion2012,
  langid = {english},
  title = {Mapping of {{Ice Motion}} in {{Antarctica Using Synthetic}}-{{Aperture Radar Data}}},
  volume = {4},
  issn = {2072-4292},
  url = {http://www.mdpi.com/2072-4292/4/9/2753},
  doi = {10.3390/rs4092753},
  number = {12},
  journaltitle = {Remote Sensing},
  urldate = {2018-02-22},
  date = {2012-09-18},
  pages = {2753-2767},
  author = {Mouginot, Jeremie and Scheuchl, Bernd and Rignot, Eric}
}

@misc{MartosAntarcticgeothermalheat2017,
  langid = {english},
  title = {Antarctic Geothermal Heat Flux Distribution and Estimated {{Curie Depths}}, Links to Gridded Files, Supplement to: {{Martos}}, {{Yasmina M}}; {{Catalan}}, {{Manuel}}; {{Jordan}}, {{Tom A}}; {{Golynsky}}, {{Alexander V}}; {{Golynsky}}, {{Dmitry A}}; {{Eagles}}, {{Graeme}}; {{Vaughan}}, {{David G}} (2017): {{Heat}} Flux Distribution of {{Antarctica}} Unveiled. {{Geophysical Research Letters}}, 44(22), 11417-11426},
  shorttitle = {Antarctic Geothermal Heat Flux Distribution and Estimated {{Curie Depths}}, Links to Gridded Files, Supplement To},
  abstract = {Antarctica is the largest reservoir of ice on Earth. Understanding its ice sheet dynamics is crucial to unraveling past global climate change and making robust climatic and sea level predictions. Of the basic parameters that shape and control ice flow, the most poorly known is geothermal heat flux. Direct observations of heat flux are difficult to obtain in Antarctica, and until now continent-wide heat flux maps have only been derived from low-resolution satellite magnetic and seismological data. We present a high resolution heat flux map and associated uncertainty derived from spectral analysis of the most advanced continental compilation of airborne magnetic data. Small-scale spatial variability and features consistent with known geology are better reproduced than in previous models, between 36\% and 50\%. Our high-resolution heat-flux map and its uncertainty distribution provide an important new boundary condition to be used in studies on future subglacial hydrology, ice-sheet dynamics and sea-level change.},
  publisher = {{PANGAEA - Data Publisher for Earth \& Environmental Science}},
  date = {2017},
  author = {Martos, Yasmina M},
  doi = {10.1594/PANGAEA.882503}
}

@article{StudingerIcecoverlandscape2003,
  langid = {english},
  title = {Ice Cover, Landscape Setting, and Geological Framework of {{Lake Vostok}}, {{East Antarctica}}},
  volume = {205},
  issn = {0012821X},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0012821X02010415},
  doi = {10.1016/S0012-821X(02)01041-5},
  number = {3-4},
  journaltitle = {Earth and Planetary Science Letters},
  urldate = {2018-02-23},
  date = {2003-01},
  pages = {195-210},
  author = {Studinger, Michael and Bell, Robin E. and Karner, Garry D. and Tikku, Anahita A. and Holt, John W. and Morse, David L. and Richter, Thomas G. and Kempf, Scott D. and Peters, Matthew E. and Blankenship, Donald D. and Sweeney, Ronald E. and Rystrom, Victoria L.}
}

@article{BellLargesubglaciallakes2007,
  langid = {english},
  title = {Large Subglacial Lakes in {{East Antarctica}} at the Onset of Fast-Flowing Ice Streams},
  volume = {445},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/articles/nature05554},
  doi = {10.1038/nature05554},
  number = {7130},
  journaltitle = {Nature},
  urldate = {2018-02-23},
  date = {2007-02},
  pages = {904-907},
  author = {Bell, Robin E. and Studinger, Michael and Shuman, Christopher A. and Fahnestock, Mark A. and Joughin, Ian}
}

@article{CarterRadarbasedsubglaciallake2007,
  langid = {english},
  title = {Radar-Based Subglacial Lake Classification in {{Antarctica}}: {{ANTARCTIC SUBGLACIAL LAKES}}},
  volume = {8},
  issn = {15252027},
  url = {http://doi.wiley.com/10.1029/2006GC001408},
  doi = {10.1029/2006GC001408},
  shorttitle = {Radar-Based Subglacial Lake Classification in {{Antarctica}}},
  number = {3},
  journaltitle = {Geochemistry, Geophysics, Geosystems},
  urldate = {2018-02-23},
  date = {2007-03},
  pages = {n/a-n/a},
  author = {Carter, Sasha P. and Blankenship, Donald D. and Peters, Matthew E. and Young, Duncan A. and Holt, John W. and Morse, David L.}
}

@article{SmithConnectedsubglaciallake2017,
  langid = {english},
  title = {Connected Subglacial Lake Drainage beneath {{Thwaites Glacier}}, {{West Antarctica}}},
  volume = {11},
  issn = {1994-0424},
  url = {https://www.the-cryosphere.net/11/451/2017/},
  doi = {10.5194/tc-11-451-2017},
  number = {1},
  journaltitle = {The Cryosphere},
  urldate = {2018-02-23},
  date = {2017-02-08},
  pages = {451-467},
  author = {Smith, Benjamin E. and Gourmelen, Noel and Huth, Alexander and Joughin, Ian}
}

@article{KimActivesubglaciallakes2016,
  langid = {english},
  title = {Active Subglacial Lakes and Channelized Water Flow beneath the {{Kamb Ice Stream}}},
  volume = {10},
  issn = {1994-0424},
  url = {http://www.the-cryosphere.net/10/2971/2016/},
  doi = {10.5194/tc-10-2971-2016},
  number = {6},
  journaltitle = {The Cryosphere},
  urldate = {2018-02-23},
  date = {2016-12-01},
  pages = {2971-2980},
  author = {Kim, Byeong-Hoon and Lee, Choon-Ki and Seo, Ki-Weon and Lee, Won Sang and Scambos, Ted}
}

@article{SiegfriedThirteenyearssubglacial2018,
  langid = {english},
  title = {Thirteen Years of Subglacial Lake Activity in {{Antarctica}} from Multi-Mission Satellite Altimetry},
  issn = {0260-3055, 1727-5644},
  url = {https://www.cambridge.org/core/product/identifier/S0260305517000362/type/journal_article},
  doi = {10.1017/aog.2017.36},
  journaltitle = {Annals of Glaciology},
  urldate = {2018-02-23},
  date = {2018-01-26},
  pages = {1-14},
  author = {Siegfried, Matthew R. and Fricker, Helen A.}
}

@article{PalmerGreenlandsubglaciallakes2013,
  langid = {english},
  title = {Greenland Subglacial Lakes Detected by Radar: {{GREENLAND SUBGLACIAL LAKES DISCOVERED}}},
  volume = {40},
  issn = {00948276},
  url = {http://doi.wiley.com/10.1002/2013GL058383},
  doi = {10.1002/2013GL058383},
  shorttitle = {Greenland Subglacial Lakes Detected by Radar},
  number = {23},
  journaltitle = {Geophysical Research Letters},
  urldate = {2018-02-23},
  date = {2013-12-16},
  pages = {6154-6159},
  author = {Palmer, Steven J. and Dowdeswell, Julian A. and Christoffersen, Poul and Young, Duncan A. and Blankenship, Donald D. and Greenbaum, Jamin S. and Benham, Toby and Bamber, Jonathan and Siegert, Martin J.}
}

@article{ScheinertNewAntarcticgravity2016,
  langid = {english},
  title = {New {{Antarctic}} Gravity Anomaly Grid for Enhanced Geodetic and Geophysical Studies in {{Antarctica}}: {{NEW ANTARCTIC GRAVITY ANOMALY GRID}}},
  volume = {43},
  issn = {00948276},
  url = {http://doi.wiley.com/10.1002/2015GL067439},
  doi = {10.1002/2015GL067439},
  shorttitle = {New {{Antarctic}} Gravity Anomaly Grid for Enhanced Geodetic and Geophysical Studies in {{Antarctica}}},
  number = {2},
  journaltitle = {Geophysical Research Letters},
  urldate = {2018-02-25},
  date = {2016-01-28},
  pages = {600-610},
  author = {Scheinert, M. and Ferraccioli, F. and Schwabe, J. and Bell, R. and Studinger, M. and Damaske, D. and Jokat, W. and Aleshkova, N. and Jordan, T. and Leitchenkov, G. and Blankenship, D. D. and Damiani, T. M. and Young, D. and Cochran, J. R. and Richter, T. D.}
}

@misc{ScheinertAntarcticfreeaircomplete2016,
  langid = {english},
  title = {Antarctic Free-Air and Complete {{Bouguer}} Gravity Anomaly Grid, Supplement to: {{Scheinert}}, {{Mirko}}; {{Ferraccioli}}, {{Fausto}}; {{Schwabe}}, {{Joachim}}; {{Bell}}, {{Robin E}}; {{Studinger}}, {{Michael}}; {{Damaske}}, {{Detlef}}; {{Jokat}}, {{Wilfried}}; {{Aleshkova}}, {{Nadezhda D}}; {{Jordan}}, {{Tom A}}; {{Leitchenkov}}, {{German L}}; {{Blankenship}}, {{Donald D}}; {{Damiani}}, {{Theresa}}; {{Young}}, {{Duncan A}}; {{Cochran}}, {{James R}}; {{Richter}}, {{Thomas}} (2016): {{New Antarctic}} Gravity Anomaly Grid for Enhanced Geodetic and Geophysical Studies in {{Antarctica}}. {{Geophysical Research Letters}}, 43(2), 600-610},
  shorttitle = {Antarctic Free-Air and Complete {{Bouguer}} Gravity Anomaly Grid, Supplement To},
  abstract = {Gravity surveying is challenging in Antarctica because of its hostile environment and inaccessibility. Nevertheless, many ground-based, airborne, and shipborne gravity campaigns have been completed by the geophysical and geodetic communities since the 1980s. We present the first modern Antarctic-wide gravity data compilation derived from 13 million data points covering an area of 10 million km**2, which corresponds to 73\% coverage of the continent. The remove-compute-restore technique was applied for gridding, which facilitated leveling of the different gravity data sets with respect to an Earth gravity model derived from satellite data alone. The resulting free-air and Bouguer gravity anomaly grids of 10 km resolution are publicly available. These grids will enable new high-resolution combined Earth gravity models to be derived and represent a major step forward toward solving the geodetic polar data gap problem. They provide a new tool to investigate continental-scale lithospheric structure and geological evolution of Antarctica.},
  publisher = {{PANGAEA - Data Publisher for Earth \& Environmental Science}},
  date = {2016},
  author = {Scheinert, Mirko and Ferraccioli, Fausto and Schwabe, Joachim and Bell, Robin E and Studinger, Michael and Damaske, Detlef and Jokat, Wilfried and Aleshkova, Nadezhda D and Jordan, Tom A and Leitchenkov, German L and Blankenship, Donald D and Damiani, Theresa and Young, Duncan A and Cochran, James R and Richter, Thomas},
  doi = {10.1594/PANGAEA.848168}
}

@article{Hirtnewdegree2190102016,
  langid = {english},
  title = {A New Degree-2190 (10 Km Resolution) Gravity Field Model for {{Antarctica}} Developed from {{GRACE}}, {{GOCE}} and {{Bedmap2}} Data},
  volume = {90},
  issn = {0949-7714, 1432-1394},
  url = {http://link.springer.com/10.1007/s00190-015-0857-6},
  doi = {10.1007/s00190-015-0857-6},
  number = {2},
  journaltitle = {Journal of Geodesy},
  urldate = {2018-02-25},
  date = {2016-02},
  pages = {105-127},
  author = {Hirt, Christian and Rexer, Moritz and Scheinert, Mirko and Pail, Roland and Claessens, Sten and Holmes, Simon}
}

@article{DongImageSuperResolutionUsing2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1501.00092},
  primaryClass = {cs},
  title = {Image {{Super}}-{{Resolution Using Deep Convolutional Networks}}},
  url = {http://arxiv.org/abs/1501.00092},
  abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
  urldate = {2018-03-09},
  date = {2014-12-31},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,I.2.6,I.4.5},
  author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou}
}

@inproceedings{VandalDeepSDGeneratingHigh2017,
  langid = {english},
  title = {{{DeepSD}}: {{Generating High Resolution Climate Change Projections}} through {{Single Image Super}}-{{Resolution}}},
  isbn = {978-1-4503-4887-4},
  url = {http://dl.acm.org/citation.cfm?doid=3097983.3098004},
  doi = {10.1145/3097983.3098004},
  shorttitle = {{{DeepSD}}},
  publisher = {{ACM Press}},
  urldate = {2018-03-09},
  date = {2017},
  pages = {1663-1672},
  author = {Vandal, Thomas and Kodra, Evan and Ganguly, Sangram and Michaelis, Andrew and Nemani, Ramakrishna and Ganguly, Auroop R.}
}

@article{GatysNeuralAlgorithmArtistic2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.06576},
  primaryClass = {cs, q-bio},
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  url = {http://arxiv.org/abs/1508.06576},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  urldate = {2018-03-11},
  date = {2015-08-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias}
}

@article{OlahBuildingBlocksInterpretability2018,
  title = {The {{Building Blocks}} of {{Interpretability}}},
  volume = {3},
  issn = {2476-0757},
  url = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010},
  number = {3},
  journaltitle = {Distill},
  urldate = {2018-03-11},
  date = {2018-03-06},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander}
}

@article{SzegedyIntriguingpropertiesneural2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6199},
  primaryClass = {cs},
  title = {Intriguing Properties of Neural Networks},
  url = {http://arxiv.org/abs/1312.6199},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  urldate = {2018-03-11},
  date = {2013-12-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob}
}

@article{HowardMobileNetsEfficientConvolutional2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04861},
  primaryClass = {cs},
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  url = {http://arxiv.org/abs/1704.04861},
  shorttitle = {{{MobileNets}}},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  urldate = {2018-03-11},
  date = {2017-04-16},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig}
}

@article{CholletXceptionDeepLearning2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.02357},
  primaryClass = {cs},
  title = {Xception: {{Deep Learning}} with {{Depthwise Separable Convolutions}}},
  url = {http://arxiv.org/abs/1610.02357},
  shorttitle = {Xception},
  abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  urldate = {2018-03-11},
  date = {2016-10-07},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chollet, François}
}

@thesis{SifreRigidMotionScatteringImage2014,
  location = {{France}},
  title = {Rigid-{{Motion Scattering For Image Classification}}},
  abstract = {Image classification is the problem of assigning a label that best describes the content of unknown images, given a set of training images with known labels. This thesis introduces image classification algorithms based on the scattering transform, studies their properties and describes extensive classification experiments on challenging texture and object image datasets. Images are high dimensional signals for which generic machine learning algorithms fail when applied directly on the raw pixel space. Therefore, most successful approaches involve building a specific low dimensional representation on which the classification is performed. Traditionally, the representation was engineered to reduce the dimensionality of images by building invariance to geometric transformations while retaining discriminative features. More recently, deep convolutional networks have achieved state-of-the-art results on most image classification tasks. Such networks progressively build more invariant representations through a hierarchy of convolutional layers where all the weights are learned. This thesis proposes several scattering representations. Those scattering representa-tions have a structure similar to convolutional networks, but the weights of scattering are},
  institution = {{Ecole Polytechnique, CMAP}},
  type = {PhD Tnesis},
  date = {2014},
  author = {Sifre, Laurent}
}

@article{SchmittDataFusionRemote2016,
  title = {Data {{Fusion}} and {{Remote Sensing}}: {{An}} Ever-Growing Relationship},
  volume = {4},
  issn = {2168-6831, 2473-2397},
  url = {http://ieeexplore.ieee.org/document/7740215/},
  doi = {10.1109/MGRS.2016.2561021},
  shorttitle = {Data {{Fusion}} and {{Remote Sensing}}},
  number = {4},
  journaltitle = {IEEE Geoscience and Remote Sensing Magazine},
  urldate = {2018-03-11},
  date = {2016-12},
  pages = {6-23},
  author = {Schmitt, Michael and Zhu, Xiao Xiang}
}

@book{Cuffeyphysicsglaciers2010,
  location = {{Burlington, MA}},
  title = {The Physics of Glaciers},
  edition = {4th ed},
  isbn = {978-0-12-369461-4},
  pagetotal = {693},
  publisher = {{Butterworth-Heinemann/Elsevier}},
  date = {2010},
  keywords = {Glaciers},
  author = {Cuffey, Kurt and Paterson, W. S. B.},
  note = {OCLC: ocn488732494}
}

@article{IkenUpliftUnteraargletscherBeginning1983,
  langid = {english},
  title = {The {{Uplift}} of {{Unteraargletscher}} at the {{Beginning}} of the {{Melt Season}}—{{A Consequence}} of {{Water Storage}} at the {{Bed}}?},
  volume = {29},
  issn = {0022-1430, 1727-5652},
  url = {https://www.cambridge.org/core/product/identifier/S0022143000005128/type/journal_article},
  doi = {10.3189/S0022143000005128},
  number = {101},
  journaltitle = {Journal of Glaciology},
  urldate = {2018-05-21},
  date = {1983},
  pages = {28-47},
  author = {Iken, A. and Röthlisberger, H. and Flotron, A. and Haeberli, W.}
}

@misc{MatsuokaQuantarctica2018,
  title = {Quantarctica},
  url = {https://data.npolar.no/dataset/8516e961-81db-4120-af13-b8a2ffe174c9},
  abstract = {Quantarctica is a collection of Antarctic geographical datasets which works with the free, cross-platform, open-source software QGIS. It includes community-contributed, peer-reviewed data from ten different scientific themes and a professionally-designed basemap.},
  publisher = {{Norwegian Polar Institute}},
  urldate = {2018-05-30},
  date = {2018},
  author = {Matsuoka, Kenichi and Skoglund, Anders and Roth, George},
  doi = {10.21334/npolar.2018.8516e961}
}

@article{KingslakeWidespreadmovementmeltwater2017,
  title = {Widespread Movement of Meltwater onto and across {{Antarctic}} Ice Shelves},
  volume = {544},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/nature22049},
  doi = {10.1038/nature22049},
  number = {7650},
  journaltitle = {Nature},
  urldate = {2018-07-02},
  date = {2017-04-19},
  pages = {349-352},
  author = {Kingslake, Jonathan and Ely, Jeremy C. and Das, Indrani and Bell, Robin E.}
}

@article{SiegertRecentadvancesunderstanding2016,
  langid = {english},
  title = {Recent Advances in Understanding {{Antarctic}} Subglacial Lakes and Hydrology},
  volume = {374},
  issn = {1364-503X, 1471-2962},
  url = {http://rsta.royalsocietypublishing.org/lookup/doi/10.1098/rsta.2014.0306},
  doi = {10.1098/rsta.2014.0306},
  number = {2059},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  urldate = {2018-07-03},
  date = {2016-01-28},
  pages = {20140306},
  author = {Siegert, Martin J. and Ross, Neil and Le Brocq, Anne M.}
}

@article{FlowersModellingwaterflow2015,
  langid = {english},
  title = {Modelling Water Flow under Glaciers and Ice Sheets},
  volume = {471},
  issn = {1364-5021, 1471-2946},
  url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.2014.0907},
  doi = {10.1098/rspa.2014.0907},
  number = {2176},
  journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  urldate = {2018-07-03},
  date = {2015-03-04},
  pages = {20140907-20140907},
  author = {Flowers, G. E.}
}

@article{MullerVelocityfluctuationswater1973,
  title = {Velocity Fluctuations and Water Regime of {{Arctic}} Valley Glaciers},
  volume = {95},
  abstract = {Over a period of 10 years, 1959-69, surface movement, ablation and, to some extent, run-off were measured in the ablation area of cold valley glaciers, particularly White Glacier on Axel Heiberg Island (lat. 80° N.) in the Canadian Arctic Archipelago. The annual, seasonal and short-period velocity fluctuations were found to be most strongly linked to the discharge capacity of the intra and subglacial drainage system. However, changes in glacier thickness, ice temperature near the bed and longitudinal stress gradient are also influential. Intraglacial, water-activated glide planes were observed.},
  journaltitle = {International Association of Scientific Hydrology Publication},
  date = {1973-01-01},
  pages = {165-182},
  author = {Müller, Fritz and Iken, Almut}
}

@thesis{BernalesCouplingAntarcticice2018,
  langid = {english},
  location = {{Germany}},
  title = {Coupling between the {{Antarctic}} Ice Flow, Subglacial Regimes and Regional Climate Conditions},
  url = {https://refubium.fu-berlin.de/handle/fub188/9581},
  institution = {{Freie Universität Berlin}},
  urldate = {2018-07-04},
  date = {2018},
  author = {Bernales, Jorge}
}

@article{Kyrke-SmithSubglacialhydrologyformation2013,
  langid = {english},
  title = {Subglacial Hydrology and the Formation of Ice Streams},
  volume = {470},
  issn = {1364-5021, 1471-2946},
  url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.2013.0494},
  doi = {10.1098/rspa.2013.0494},
  number = {2161},
  journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  urldate = {2018-07-04},
  date = {2013-11-06},
  pages = {20130494-20130494},
  author = {Kyrke-Smith, T. M. and Katz, R. F. and Fowler, A. C.}
}

@article{WinsborrowWhatcontrolslocation2010,
  langid = {english},
  title = {What Controls the Location of Ice Streams?},
  volume = {103},
  issn = {00128252},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0012825210000838},
  doi = {10.1016/j.earscirev.2010.07.003},
  number = {1-2},
  journaltitle = {Earth-Science Reviews},
  urldate = {2018-07-04},
  date = {2010-11},
  pages = {45-59},
  author = {Winsborrow, Monica C.M. and Clark, Chris D. and Stokes, Chris R.}
}

@article{BlankenshipSeismicmeasurementsreveal1986,
  langid = {english},
  title = {Seismic Measurements Reveal a Saturated Porous Layer beneath an Active {{Antarctic}} Ice Stream},
  volume = {322},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/322054a0},
  doi = {10.1038/322054a0},
  number = {6074},
  journaltitle = {Nature},
  urldate = {2018-07-04},
  date = {1986-07},
  pages = {54-57},
  author = {Blankenship, D. D. and Bentley, C. R. and Rooney, S. T. and Alley, R. B.}
}

@article{AlleyDeformationtillice1986,
  langid = {english},
  title = {Deformation of till beneath Ice Stream {{B}}, {{West Antarctica}}},
  volume = {322},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/322057a0},
  doi = {10.1038/322057a0},
  number = {6074},
  journaltitle = {Nature},
  urldate = {2018-07-04},
  date = {1986-07},
  pages = {57-59},
  author = {Alley, R. B. and Blankenship, D. D. and Bentley, C. R. and Rooney, S. T.}
}


