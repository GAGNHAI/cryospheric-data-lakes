
@article{Smithinventoryactivesubglacial2009,
  langid = {english},
  title = {An Inventory of Active Subglacial Lakes in {{Antarctica}} Detected by {{ICESat}} (2003–2008)},
  volume = {55},
  issn = {0022-1430, 1727-5652},
  url = {https://www.cambridge.org/core/product/identifier/S0022143000207399/type/journal_article},
  doi = {10.3189/002214309789470879},
  number = {192},
  journaltitle = {Journal of Glaciology},
  urldate = {2018-01-10},
  date = {2009},
  pages = {573--595},
  author = {Smith, Benjamin E. and Fricker, Helen A. and Joughin, Ian R. and Tulaczyk, Slawek}
}

@article{WrightfourthinventoryAntarctic2012,
  langid = {english},
  title = {A Fourth Inventory of {{Antarctic}} Subglacial Lakes},
  volume = {24},
  issn = {0954-1020, 1365-2079},
  url = {http://www.journals.cambridge.org/abstract_S095410201200048X},
  doi = {10.1017/S095410201200048X},
  issue = {06},
  journaltitle = {Antarctic Science},
  urldate = {2018-01-10},
  date = {2012-12},
  pages = {659--664},
  author = {Wright, Andrew and Siegert, Martin}
}

@article{RedmonYOLO9000BetterFaster2016,
  langid = {english},
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  url = {https://arxiv.org/abs/1612.08242},
  shorttitle = {{{YOLO9000}}},
  urldate = {2018-01-10},
  date = {2016-12-25},
  author = {Redmon, Joseph and Farhadi, Ali}
}

@article{SilverSoftwaresimplified2017,
  title = {Software Simplified},
  volume = {546},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/546173a},
  doi = {10.1038/546173a},
  number = {7656},
  journaltitle = {Nature},
  urldate = {2018-01-10},
  date = {2017-05-29},
  pages = {173--174},
  author = {Silver, Andrew}
}

@article{BoettigerintroductionDockerreproducible2015,
  langid = {english},
  title = {An Introduction to {{Docker}} for Reproducible Research},
  volume = {49},
  issn = {01635980},
  url = {http://dl.acm.org/citation.cfm?doid=2723872.2723882},
  doi = {10.1145/2723872.2723882},
  number = {1},
  journaltitle = {ACM SIGOPS Operating Systems Review},
  urldate = {2018-01-10},
  date = {2015-01-20},
  pages = {71--79},
  author = {Boettiger, Carl}
}

@article{CrickReproducibilityResearchSystems2017,
  langid = {english},
  title = {Reproducibility in {{Research}}: {{Systems}}, {{Infrastructure}}, {{Culture}}},
  volume = {5},
  issn = {2049-9647},
  url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.73/},
  doi = {10.5334/jors.73},
  shorttitle = {Reproducibility in {{Research}}},
  journaltitle = {Journal of Open Research Software},
  urldate = {2018-01-14},
  date = {2017-11-09},
  author = {Crick, Tom and Hall, Benjamin A. and Ishtiaq, Samin}
}

@article{PerkelDemocraticdatabasesscience2016,
  title = {Democratic Databases: Science on {{GitHub}}},
  volume = {538},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/doifinder/10.1038/538127a},
  doi = {10.1038/538127a},
  shorttitle = {Democratic Databases},
  number = {7623},
  journaltitle = {Nature},
  urldate = {2018-01-15},
  date = {2016-10-03},
  pages = {127--128},
  author = {Perkel, Jeffrey}
}

@article{BlischakQuickIntroductionVersion2016,
  langid = {english},
  title = {A {{Quick Introduction}} to {{Version Control}} with {{Git}} and {{GitHub}}},
  volume = {12},
  issn = {1553-7358},
  url = {http://dx.plos.org/10.1371/journal.pcbi.1004668},
  doi = {10.1371/journal.pcbi.1004668},
  number = {1},
  journaltitle = {PLOS Computational Biology},
  urldate = {2018-01-15},
  date = {2016-01-19},
  pages = {e1004668},
  author = {Blischak, John D. and Davenport, Emily R. and Wilson, Greg},
  editor = {Ouellette, Francis}
}

@software{OgdenDatDistributedDataset2018,
  title = {Dat  - {{Distributed Dataset Synchronization}} and {{Versioning}}},
  url = {https://github.com/datprotocol/whitepaper/blob/master/dat-paper.pdf},
  version = {2.0},
  publisher = {{Datproject}},
  urldate = {2018-01-15},
  date = {2018-01-12},
  author = {Ogden, Maxwell and McKelvey, Karissa and Madsen, Mathias Buus and {Code for Science}},
  origdate = {2018-01-11T19:09:45Z}
}

@article{MykletunAuthenticationintegrityoutsourced2006,
  langid = {english},
  title = {Authentication and Integrity in Outsourced Databases},
  volume = {2},
  issn = {15533077},
  url = {http://portal.acm.org/citation.cfm?doid=1149976.1149977},
  doi = {10.1145/1149976.1149977},
  number = {2},
  journaltitle = {ACM Transactions on Storage},
  urldate = {2018-01-15},
  date = {2006-05-01},
  pages = {107--138},
  author = {Mykletun, Einar and Narasimha, Maithili and Tsudik, Gene}
}

@incollection{AumassonBLAKE2SimplerSmaller2013,
  location = {{Berlin, Heidelberg}},
  title = {{{BLAKE2}}: {{Simpler}}, {{Smaller}}, {{Fast}} as {{MD5}}},
  volume = {7954},
  isbn = {978-3-642-38979-5 978-3-642-38980-1},
  url = {http://link.springer.com/10.1007/978-3-642-38980-1_8},
  shorttitle = {{{BLAKE2}}},
  booktitle = {Applied {{Cryptography}} and {{Network Security}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2018-01-15},
  date = {2013},
  pages = {119--135},
  author = {Aumasson, Jean-Philippe and Neves, Samuel and Wilcox-O’Hearn, Zooko and Winnerlein, Christian},
  editor = {Jacobson, Michael and Locasto, Michael and Mohassel, Payman and Safavi-Naini, Reihaneh},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  doi = {10.1007/978-3-642-38980-1_8}
}

@article{BernsteinHighspeedhighsecuritysignatures2012,
  langid = {english},
  title = {High-Speed High-Security Signatures},
  volume = {2},
  issn = {2190-8508, 2190-8516},
  url = {http://link.springer.com/10.1007/s13389-012-0027-1},
  doi = {10.1007/s13389-012-0027-1},
  number = {2},
  journaltitle = {Journal of Cryptographic Engineering},
  urldate = {2018-01-15},
  date = {2012-09},
  pages = {77--89},
  author = {Bernstein, Daniel J. and Duif, Niels and Lange, Tanja and Schwabe, Peter and Yang, Bo-Yin}
}

@incollection{MaymounkovKademliaPeertoPeerInformation2002,
  location = {{Berlin, Heidelberg}},
  title = {Kademlia: {{A Peer}}-to-{{Peer Information System Based}} on the {{XOR Metric}}},
  volume = {2429},
  isbn = {978-3-540-44179-3 978-3-540-45748-0},
  url = {http://link.springer.com/10.1007/3-540-45748-8_5},
  shorttitle = {Kademlia},
  booktitle = {Peer-to-{{Peer Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2018-01-15},
  date = {2002},
  pages = {53--65},
  author = {Maymounkov, Petar and Mazières, David},
  editor = {Druschel, Peter and Kaashoek, Frans and Rowstron, Antony},
  editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
  editorbtype = {redactor},
  options = {useprefix=true},
  doi = {10.1007/3-540-45748-8_5}
}

@article{chollet2015keras,
  title = {Keras},
  url = {https://github.com/keras-team/keras},
  date = {2015},
  author = {Chollet, François and {others}}
}

@article{RonnebergerUNetConvolutionalNetworks2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.04597},
  primaryClass = {cs},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  url = {http://arxiv.org/abs/1505.04597},
  shorttitle = {U-{{Net}}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  urldate = {2018-01-26},
  date = {2015-05-18},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}
}

@article{LeCunBackpropagationAppliedHandwritten1989,
  langid = {english},
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  volume = {1},
  issn = {0899-7667, 1530-888X},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.541},
  doi = {10.1162/neco.1989.1.4.541},
  number = {4},
  journaltitle = {Neural Computation},
  urldate = {2018-01-26},
  date = {1989-12},
  pages = {541--551},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.}
}

@article{LecunGradientbasedlearningapplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  volume = {86},
  issn = {00189219},
  url = {http://ieeexplore.ieee.org/document/726791/},
  doi = {10.1109/5.726791},
  number = {11},
  journaltitle = {Proceedings of the IEEE},
  urldate = {2018-01-26},
  date = {1998-11},
  pages = {2278--2324},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.}
}

@article{KrizhevskyImageNetclassificationdeep2017,
  langid = {english},
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  volume = {60},
  issn = {00010782},
  url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
  doi = {10.1145/3065386},
  number = {6},
  journaltitle = {Communications of the ACM},
  urldate = {2018-01-26},
  date = {2017-05-24},
  pages = {84--90},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.}
}

@inproceedings{SteinkrausUsingGPUsmachine2005,
  title = {Using {{GPUs}} for Machine Learning Algorithms},
  isbn = {978-0-7695-2420-7},
  url = {http://ieeexplore.ieee.org/document/1575717/},
  doi = {10.1109/ICDAR.2005.251},
  publisher = {{IEEE}},
  urldate = {2018-01-26},
  date = {2005},
  pages = {1115--1120 Vol. 2},
  author = {Steinkraus, D. and Buck, I. and Simard, P.Y.}
}

@article{SimonyanVeryDeepConvolutional2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  primaryClass = {cs},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  url = {http://arxiv.org/abs/1409.1556},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  urldate = {2018-01-26},
  date = {2014-09-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew}
}

@article{GirshickRichfeaturehierarchies2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.2524},
  primaryClass = {cs},
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  url = {http://arxiv.org/abs/1311.2524},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/\textasciitilde{}rbg/rcnn.},
  urldate = {2018-01-26},
  date = {2013-11-11},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}
}

@article{GirshickFastRCNN2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.08083},
  primaryClass = {cs},
  title = {Fast {{R}}-{{CNN}}},
  url = {http://arxiv.org/abs/1504.08083},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  urldate = {2018-01-26},
  date = {2015-04-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Girshick, Ross}
}

@article{RenFasterRCNNRealTime2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.01497},
  primaryClass = {cs},
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  url = {http://arxiv.org/abs/1506.01497},
  shorttitle = {Faster {{R}}-{{CNN}}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  urldate = {2018-01-26},
  date = {2015-06-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian}
}

@article{RedmonYouOnlyLook2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02640},
  primaryClass = {cs},
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  url = {http://arxiv.org/abs/1506.02640},
  shorttitle = {You {{Only Look Once}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  urldate = {2018-01-26},
  date = {2015-06-08},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali}
}

@article{HeMaskRCNN2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06870},
  primaryClass = {cs},
  title = {Mask {{R}}-{{CNN}}},
  url = {http://arxiv.org/abs/1703.06870},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  urldate = {2018-01-26},
  date = {2017-03-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross}
}

@article{LongFullyConvolutionalNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.4038},
  primaryClass = {cs},
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  url = {http://arxiv.org/abs/1411.4038},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  urldate = {2018-01-28},
  date = {2014-11-14},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor}
}

@article{JegouOneHundredLayers2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.09326},
  primaryClass = {cs},
  title = {The {{One Hundred Layers Tiramisu}}: {{Fully Convolutional DenseNets}} for {{Semantic Segmentation}}},
  url = {http://arxiv.org/abs/1611.09326},
  shorttitle = {The {{One Hundred Layers Tiramisu}}},
  abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
  urldate = {2018-01-28},
  date = {2016-11-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua}
}

@article{VisinReNetRecurrentNeural2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.00393},
  primaryClass = {cs},
  title = {{{ReNet}}: {{A Recurrent Neural Network Based Alternative}} to {{Convolutional Networks}}},
  url = {http://arxiv.org/abs/1505.00393},
  shorttitle = {{{ReNet}}},
  abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
  urldate = {2018-01-29},
  date = {2015-05-03},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua}
}

@article{VisinReSegRecurrentNeural2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.07053},
  primaryClass = {cs},
  title = {{{ReSeg}}: {{A Recurrent Neural Network}}-Based {{Model}} for {{Semantic Segmentation}}},
  url = {http://arxiv.org/abs/1511.07053},
  shorttitle = {{{ReSeg}}},
  abstract = {We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg.},
  urldate = {2018-01-29},
  date = {2015-11-22},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Visin, Francesco and Ciccone, Marco and Romero, Adriana and Kastner, Kyle and Cho, Kyunghyun and Bengio, Yoshua and Matteucci, Matteo and Courville, Aaron}
}

@article{LiuSSDSingleShot2015,
  langid = {english},
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  url = {https://arxiv.org/abs/1512.02325v5},
  doi = {10.1007/978-3-319-46448-0_2},
  shorttitle = {{{SSD}}},
  urldate = {2018-01-29},
  date = {2015-12-08},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.}
}

@article{HeDeepResidualLearning2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  primaryClass = {cs},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  url = {http://arxiv.org/abs/1512.03385},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  urldate = {2018-01-29},
  date = {2015-12-10},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}
}

@article{HuangDenselyConnectedConvolutional2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.06993},
  primaryClass = {cs},
  title = {Densely {{Connected Convolutional Networks}}},
  url = {http://arxiv.org/abs/1608.06993},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
  urldate = {2018-01-29},
  date = {2016-08-24},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and van der Maaten, Laurens},
  options = {useprefix=true}
}

@article{ZhangRoadExtractionDeep2017,
  langid = {english},
  title = {Road {{Extraction}} by {{Deep Residual U}}-{{Net}}},
  url = {https://arxiv.org/abs/1711.10684},
  urldate = {2018-01-29},
  date = {2017-11-29},
  author = {Zhang, Zhengxin and Liu, Qingjie and Wang, Yunhong}
}

@article{LiDeepUNetDeepFully2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.00201},
  primaryClass = {cs},
  title = {{{DeepUNet}}: {{A Deep Fully Convolutional Network}} for {{Pixel}}-Level {{Sea}}-{{Land Segmentation}}},
  url = {http://arxiv.org/abs/1709.00201},
  shorttitle = {{{DeepUNet}}},
  abstract = {Semantic segmentation is a fundamental research in remote sensing image processing. Because of the complex maritime environment, the sea-land segmentation is a challenging task. Although the neural network has achieved excellent performance in semantic segmentation in the last years, there are a few of works using CNN for sea-land segmentation and the results could be further improved. This paper proposes a novel deep convolution neural network named DeepUNet. Like the U-Net, its structure has a contracting path and an expansive path to get high resolution output. But differently, the DeepUNet uses DownBlocks instead of convolution layers in the contracting path and uses UpBlock in the expansive path. The two novel blocks bring two new connections that are U-connection and Plus connection. They are promoted to get more precise segmentation results. To verify our network architecture, we made a new challenging sea-land dataset and compare the DeepUNet on it with the SegNet and the U-Net. Experimental results show that DeepUNet achieved good performance compared with other architectures, especially in high-resolution remote sensing imagery.},
  urldate = {2018-01-29},
  date = {2017-09-01},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Li, Ruirui and Liu, Wenjie and Yang, Lei and Sun, Shihao and Hu, Wei and Zhang, Fan and Li, Wei}
}

@article{ChenDeepLabSemanticImage2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.00915},
  primaryClass = {cs},
  title = {{{DeepLab}}: {{Semantic Image Segmentation}} with {{Deep Convolutional Nets}}, {{Atrous Convolution}}, and {{Fully Connected CRFs}}},
  url = {http://arxiv.org/abs/1606.00915},
  shorttitle = {{{DeepLab}}},
  abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  urldate = {2018-01-29},
  date = {2016-06-02},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.}
}

@article{ChenRethinkingAtrousConvolution2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.05587},
  primaryClass = {cs},
  title = {Rethinking {{Atrous Convolution}} for {{Semantic Image Segmentation}}},
  url = {http://arxiv.org/abs/1706.05587},
  abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  urldate = {2018-01-29},
  date = {2017-06-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig}
}

@article{SabourDynamicRoutingCapsules2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.09829},
  primaryClass = {cs},
  title = {Dynamic {{Routing Between Capsules}}},
  url = {http://arxiv.org/abs/1710.09829},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  urldate = {2018-01-29},
  date = {2017-10-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.}
}

@article{ZhuDeepLearningRemote2017,
  title = {Deep {{Learning}} in {{Remote Sensing}}: {{A Comprehensive Review}} and {{List}} of {{Resources}}},
  volume = {5},
  issn = {2168-6831, 2473-2397},
  url = {http://ieeexplore.ieee.org/document/8113128/},
  doi = {10.1109/MGRS.2017.2762307},
  shorttitle = {Deep {{Learning}} in {{Remote Sensing}}},
  number = {4},
  journaltitle = {IEEE Geoscience and Remote Sensing Magazine},
  urldate = {2018-01-31},
  date = {2017-12},
  pages = {8--36},
  author = {Zhu, Xiao Xiang and Tuia, Devis and Mou, Lichao and Xia, Gui-Song and Zhang, Liangpei and Xu, Feng and Fraundorfer, Friedrich}
}

@article{XiaDOTALargescaleDataset2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.10398},
  primaryClass = {cs},
  title = {{{DOTA}}: {{A Large}}-Scale {{Dataset}} for {{Object Detection}} in {{Aerial Images}}},
  url = {http://arxiv.org/abs/1711.10398},
  shorttitle = {{{DOTA}}},
  abstract = {Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect \$2806\$ aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using \$15\$ common object categories. The fully annotated DOTA images contains \$188,282\$ instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.},
  urldate = {2018-01-31},
  date = {2017-11-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Xia, Gui-Song and Bai, Xiang and Ding, Jian and Zhu, Zhen and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei}
}

@article{Liu3DCNNDQNRNNDeepReinforcement2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.06783},
  primaryClass = {cs},
  title = {{{3DCNN}}-{{DQN}}-{{RNN}}: {{A Deep Reinforcement Learning Framework}} for {{Semantic Parsing}} of {{Large}}-Scale {{3D Point Clouds}}},
  url = {http://arxiv.org/abs/1707.06783},
  shorttitle = {{{3DCNN}}-{{DQN}}-{{RNN}}},
  abstract = {Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN) for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the object class efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.},
  urldate = {2018-01-31},
  date = {2017-07-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Liu, Fangyu and Li, Shuaipeng and Zhang, Liqiang and Zhou, Chenghu and Ye, Rongtian and Wang, Yuebin and Lu, Jiwen}
}

@article{MinhDeepRecurrentNeural2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.03694},
  primaryClass = {cs},
  title = {Deep {{Recurrent Neural Networks}} for Mapping Winter Vegetation Quality Coverage via Multi-Temporal {{SAR Sentinel}}-1},
  url = {http://arxiv.org/abs/1708.03694},
  abstract = {Mapping winter vegetation quality coverage is a challenge problem of remote sensing. This is due to the cloud coverage in winter period, leading to use radar rather than optical images. The objective of this paper is to provide a better understanding of the capabilities of radar Sentinel-1 and deep learning concerning about mapping winter vegetation quality coverage. The analysis presented in this paper is carried out on multi-temporal Sentinel-1 data over the site of La Rochelle, France, during the campaign in December 2016. This dataset were processed in order to produce an intensity radar data stack from October 2016 to February 2017. Two deep Recurrent Neural Network (RNN) based classifier methods were employed. We found that the results of RNNs clearly outperformed the classical machine learning approaches (Support Vector Machine and Random Forest). This study confirms that the time series radar Sentinel-1 and RNNs could be exploited for winter vegetation quality cover mapping.},
  urldate = {2018-01-31},
  date = {2017-08-11},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Minh, Dinh Ho Tong and Ienco, Dino and Gaetano, Raffaele and Lalande, Nathalie and Ndikumana, Emile and Osman, Faycal and Maurel, Pierre}
}

@article{IencoLandCoverClassification2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04055},
  title = {Land {{Cover Classification}} via {{Multi}}-Temporal {{Spatial Data}} by {{Recurrent Neural Networks}}},
  volume = {14},
  issn = {1545-598X, 1558-0571},
  url = {http://arxiv.org/abs/1704.04055},
  doi = {10.1109/LGRS.2017.2728698},
  abstract = {Nowadays, modern earth observation programs produce huge volumes of satellite images time series (SITS) that can be useful to monitor geographical areas through time. How to efficiently analyze such kind of information is still an open question in the remote sensing field. Recently, deep learning methods proved suitable to deal with remote sensing data mainly for scene classification (i.e. Convolutional Neural Networks - CNNs - on single images) while only very few studies exist involving temporal deep learning approaches (i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series. In this letter we evaluate the ability of Recurrent Neural Networks, in particular the Long-Short Term Memory (LSTM) model, to perform land cover classification considering multi-temporal spatial data derived from a time series of satellite images. We carried out experiments on two different datasets considering both pixel-based and object-based classification. The obtained results show that Recurrent Neural Networks are competitive compared to state-of-the-art classifiers, and may outperform classical approaches in presence of low represented and/or highly mixed classes. We also show that using the alternative feature representation generated by LSTM can improve the performances of standard classifiers.},
  number = {10},
  journaltitle = {IEEE Geoscience and Remote Sensing Letters},
  urldate = {2018-01-31},
  date = {2017-10},
  pages = {1685--1689},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  author = {Ienco, Dino and Gaetano, Raffaele and Dupaquier, Claire and Maurel, Pierre}
}

@article{ZhangProgressivelyDiffusedNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.05839},
  primaryClass = {cs},
  title = {Progressively {{Diffused Networks}} for {{Semantic Image Segmentation}}},
  url = {http://arxiv.org/abs/1702.05839},
  abstract = {This paper introduces Progressively Diffused Networks (PDNs) for unifying multi-scale context modeling with deep feature learning, by taking semantic image segmentation as an exemplar application. Prior neural networks, such as ResNet, tend to enhance representational power by increasing the depth of architectures and driving the training objective across layers. However, we argue that spatial dependencies in different layers, which generally represent the rich contexts among data elements, are also critical to building deep and discriminative representations. To this end, our PDNs enables to progressively broadcast information over the learned feature maps by inserting a stack of information diffusion layers, each of which exploits multi-dimensional convolutional LSTMs (Long-Short-Term Memory Structures). In each LSTM unit, a special type of atrous filters are designed to capture the short range and long range dependencies from various neighbors to a certain site of the feature map and pass the accumulated information to the next layer. From the extensive experiments on semantic image segmentation benchmarks (e.g., ImageNet Parsing, PASCAL VOC2012 and PASCAL-Part), our framework demonstrates the effectiveness to substantially improve the performances over the popular existing neural network models, and achieves state-of-the-art on ImageNet Parsing for large scale semantic segmentation.},
  urldate = {2018-01-31},
  date = {2017-02-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Zhang, Ruimao and Yang, Wei and Peng, Zhanglin and Wang, Xiaogang and Lin, Liang}
}

@article{YuDilatedResidualNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09914},
  primaryClass = {cs},
  title = {Dilated {{Residual Networks}}},
  url = {http://arxiv.org/abs/1705.09914},
  abstract = {Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model's depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (`degridding'), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.},
  urldate = {2018-01-31},
  date = {2017-05-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Yu, Fisher and Koltun, Vladlen and Funkhouser, Thomas}
}

@incollection{ZhangImageSegmentationPyramid2017,
  location = {{Cham}},
  title = {Image {{Segmentation}} with {{Pyramid Dilated Convolution Based}} on {{ResNet}} and {{U}}-{{Net}}},
  volume = {10635},
  isbn = {978-3-319-70095-3 978-3-319-70096-0},
  url = {http://link.springer.com/10.1007/978-3-319-70096-0_38},
  booktitle = {Neural {{Information Processing}}},
  publisher = {{Springer International Publishing}},
  urldate = {2018-01-31},
  date = {2017},
  pages = {364--372},
  author = {Zhang, Qiao and Cui, Zhipeng and Niu, Xiaoguang and Geng, Shijie and Qiao, Yu},
  editor = {Liu, Derong and Xie, Shengli and Li, Yuanqing and Zhao, Dongbin and El-Alfy, El-Sayed M.},
  doi = {10.1007/978-3-319-70096-0_38}
}

@article{HuangSpeedaccuracytradeoffs2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.10012},
  primaryClass = {cs},
  title = {Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors},
  url = {http://arxiv.org/abs/1611.10012},
  abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
  urldate = {2018-02-04},
  date = {2016-11-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin}
}


